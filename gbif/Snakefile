#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import xml.etree.ElementTree as ET
import zipfile

import snakemake
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider


configfile: 'config.yaml'

HTTP = HTTPRemoteProvider()


rule all:
    input:
        expand('data/processed/{dataset}.csv', dataset=config.keys()),
        'data/processed/datasets.tsv'


rule download_data:
    input:
        lambda w: HTTP.remote(
            'www.gbif.se/ipt/archive.do?r=' + w.dataset,
            insecure=True, keep_local=False)
    output:
        'data/raw/{dataset}/occurrence.txt',
        'data/raw/{dataset}/eml.xml',
    run:
        z = zipfile.ZipFile(input[0])
        d = os.path.dirname(output[0])
        z.extract('occurrence.txt', path=d)
        z.extract('eml.xml', path=d)


rule parse_dataset_info:
    input: 'data/raw/{dataset}/eml.xml'
    output: 'data/interim/datasets/{dataset}.tsv'
    run:
        tree = ET.parse(input[0])
        root = tree.getroot()
        (dataset_id, version) = root.attrib['packageId'].split('/')
        with open(output[0], 'w') as f:
            f.write(
                wildcards.dataset + '\t' + dataset_id + '\t' + version + '\n')


rule filter_sweden:
    input: 'data/raw/{dataset}/occurrence.txt'
    output: 'data/interim/filtered/{dataset}.tsv'
    params:
        dtypes = (lambda w: config[w.dataset]['columns']),
        distinct_column_set = (
            lambda w: config[w.dataset]['distinctColumnSet']),
    script: 'scripts/filter_sweden.py'


rule extract_loc_tokens:
    input: 'data/interim/filtered/{dataset}.tsv'
    output: 'data/interim/tokens/{dataset}.tsv'
    params:
        extraction_config = (lambda w: config[w.dataset]['tokensExtraction'])
    script: 'scripts/extract_loc_tokens.py'


rule prepare_pelias_csv:
    input:
        'data/interim/filtered/{dataset}.tsv',
        'data/interim/tokens/{dataset}.tsv'
    output:
        'data/processed/{dataset}.csv'
    params:
        gbif_dtypes = (lambda w: config[w.dataset]['columns']),
        tokens_cols = lambda w:
            list(config[w.dataset]['tokensExtraction'].keys()),
        name = (lambda w: config[w.dataset]['peliasName']),
        displayLabel = (lambda w: config[w.dataset]['locationDisplayLabel'])
    script: 'scripts/prepare_pelias_csv.py'


rule concatentate_dataset_metadata:
    input: expand('data/interim/datasets/{dataset}.tsv', dataset=config.keys())
    output: 'data/processed/datasets.tsv'
    shell:
        """
        printf 'dataset\tdatasetKey\tversion\n' > {output[0]}
        cat data/interim/datasets/*.tsv >> {output[0]}
        """
